# Sentimental-Analysis-Movie-Reviews
Sentiment analysis refers to the identification/recognition of sentiment in natural language. There are many interesting and challenging issues in this domain and the problem has a range of application areas. In its most basic form, the sentiment analysis challenge has a passage of text and an associated sentiment classification (positive or negative, classically) and the goal is to learn the relationships between the text and the outcomes. 
There are many difficult cases involving two or more sentiment words being used in the sentence, ambiguity, difficulty with negations etc. 
“Despite the wonderfully packaging, the product was poor”. 
“The excellent acting and soundtrack could not save this movie.”

Dataset: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. https://ai.stanford.edu/~amaas/data/sentiment/

Preprocessing: The movie review zip file was downloaded and from it the review data was extracted. The zip file contained folders with both Train and Test examples with 25,000 reviews in each folder. As the dataset was very large, we took only the Train data set which is split into a further two subfolders of positive and negative examples respectively. To merge these files, a simple command line argument was used to copy the text of each file into a single txt file. First the positive examples were merged from the ‘pos’ folder (FOR %f IN(*txt)) and copied into a new file ‘positive_examples.txt’. This was repeated for the negative examples. The final output file was the concatenated result of merging these two examples, positive instances followed by negative instances. This made it possible to add labels when the text file was loaded into the notebook.
The panadas library is used for data manipulation in python and was useful when loading the text file and adding the labels. A list was created of length 25,000 with the first 12,500 entries given the value 1 (positive sentiment) and the remaining entries given the value 0 (negative sentiment), this list was then added as a new column to the data frame. The data frame now contained a row for each review with its respective label. 
The preprocessing phase involved reducing the amount of unique word tokens. A method was created called ‘preprocess()’ was called for each row in the dataset. Each review was tokenized into its individual words. All the words were lowercased to remove duplicates, punctuation was removed as well as numbers as they add no sentiment to the sentence. The words were stemmed and finally the most common words in English, ‘stopwords’, which are contained in most sentences and so add no additional sentiment were also removed. The words were joined into as a single string so they could be given as an input to the Keras tokenizer. 
It was then necessary to tokenize the words in each sentence. This is done as it is not possible to input raw text data into the RNN model. Using the ‘ Tokenizer()’class a new tokenizer object was created and fitted on the list of pre-processed sentences. As well as this, a word dictionary which maps every unique word found in the reviews to a unique integer is created. Following this each word review is mapped to a numerical vector representing each sentence, with the words of each review being replaced with their corresponding unique integer value. These numerical vectors all have varying lengths, as some reviews are longer or shorter than others.
 In order to account for the varying lengths of reviews, as well as resolve issues relating to this difference in length, we use padding to complete our numerical word vectors. When a word occurs at the beginning of a sentence, its context is truncated. Adding padding words helps as it makes each word vector the same length and helps to detect the start/end of sentences. In the case of each numerical vector, padding consists of adding extra 0 values in positions where a certain word has not occurred. The length of each word vector is set as the length of the maximum review, i.e. the review with the most words.


Architecture: The Neural Network architecture we decided to use was a Recurrent Neural Network (RNN) with LSTM cells. RNN’s are used primarily when dealing with sequential data, with written text being a prime example of sequential data. 
With written text, each word is determined by the words preceding it, and so the chosen architecture must have some form of ‘memory’ of previous states. This memory requirement is satisfied using LSTM cells which ensure important information learned in previous states is stored in order to help predict future states. This is important as if this knowledge of previous states is not taken into consideration, the meaning or grammar of the sentence could be distorted. 
The key principle distinguishing RNN’s from other NN architectures is that in an RNN the output of each neuron is a function of the current state of the neuron (in this case the previous word) and the input vector (the current word). As well as this, RNN models can handle variable length inputs, in this case variable length movie reviews. With other architectures, fixed size inputs may be required.

Results: After training the model, using a ratio of 80:20 training data to validation data, for 100 epochs using mini batch stochastic gradient descent, the model returned a training accuracy of 91.67% with a training loss of 0.248. 
The validation data returned an accuracy of 87.82% with loss of 0.3163. This demonstrates possible overtraining but upon evaluating the model on test data a similar accuracy of 87.64% with a loss of 0.3355 was obtained, which is consistent with the validation data. 




